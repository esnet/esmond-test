#!/usr/bin/env python

"""
Generates secondary aggregation statistics from esxsnmp data base rates.
"""
import calendar
import datetime
import fcntl
import json
import os
import sys
import time

from django.conf import settings

from esxsnmp.config import get_config, get_config_path
from esxsnmp.cassandra import CASSANDRA_DB, RawData

from pycassa.columnfamily import NotFoundException

# This sets the seconds previous to time.time() to stop
# reading base rates since the most recent ones are probably
# incomplete.
RATE_TAIL_DELAY = 180

def program_lock(lockfile):
    fd = os.open(lockfile, os.O_CREAT | os.O_TRUNC | os.O_WRONLY)
    try:
        fcntl.lockf(fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
    except IOError:
        return False

    return True

def _get_rollup_freqs(oid):
    # XXX(mmg): going to need to be able to get my hands on the aggregation
    # values for the base rate and rollups.
    return [300, 1800, 7200, 86400]
    
def _agg_timestamp(data, freq):
    return datetime.datetime.utcfromtimestamp((data.ts_to_unixtime() / freq) * freq)


# Check to see if another insgtance is running.
if not program_lock(os.path.join(settings.ESXSNMP_ROOT, 'statgen.lock')):
    sys.exit()

config = get_config(get_config_path())

db = CASSANDRA_DB(config, clear_on_test=False)

keys = []
agg_freqs = {}

# Get a list of all the keys currently in the base rate column family.
for k in db.rates._column_family.get_range(column_count=0,filter_empty=False):
    keys.append(k[0])

keys.sort()

for key in keys:
    print 'processing', key
    device,path,oid,base_freq,year = key.split(RawData._key_delimiter)
    for freq in _get_rollup_freqs(oid):
    # XXX: need the frequencies for the rollup the "right way" here.
        agg_key = '%s%s%s%s%s%s%s%s%s' % \
            (device, RawData._key_delimiter,
            path, RawData._key_delimiter,
            oid, RawData._key_delimiter,
            freq, RawData._key_delimiter,
            year)
        try:
            # See if there is a corresponding aggregation row for the 
            # base rate rows.  The exception should trigger when
            # the script is first run/initialized, if a new device/path
            # is added, or if a new rollup frequency has been added to 
            # an oid.
            ret = db.stat_agg._column_family.get(agg_key)
        except NotFoundException:
            print 'no key', agg_key, 'found in stat aggregations'
            # This will go through an entire row/year's worth of data and
            # generate the rollups at the approprite frequency.
            #
            # This operation is idempotent because there is only one min/max.
            for c in db.rates._column_family.xget(key, 
                        column_finish=int(time.time()) - RATE_TAIL_DELAY):
                ts = c[0]
                is_valid = c[1]['is_valid']
                val = c[1]['val']
                if is_valid == 0:
                    continue
                data = RawData(device, None, oid, path, ts, val, base_freq)
                #db.update_stat_aggregation(data, _agg_timestamp(data, freq), freq)
        # Compare the "main" base rate key agianst its associated rollups
        # to determine the lowest rollup frequency.  This will be used to 
        # determine a "starting point" to generate ongoing stat rollups.
        if not agg_freqs.has_key(key):
            agg_freqs[key] = int(freq)
        else:
            if agg_freqs[key] > int(freq):
                agg_freqs[key] = int(freq)
        pass
    print 'updating aggs for', key
    # Get the timestamp of the last finest grained aggregate written - it will
    # be before the last base rate actually processed due to the rounding.  There
    # will be a small overlap with the previous base rates processed, but doesn't
    # matter since updating the min/max is idempotent.
    agg_key = '%s%s%s%s%s%s%s%s%s' % \
        (device, RawData._key_delimiter,
        path, RawData._key_delimiter,
        oid, RawData._key_delimiter,
        agg_freqs[key], RawData._key_delimiter,
        year)
    ret = db.stat_agg._column_family.get(agg_key, column_count=1, column_reversed=True)
    starting_ts = ret.keys()[0]
    # Read all the values from the base rate row starting with that timestamp,
    # and generate a list of data objects from the results.
    ret = db.rates._column_family.xget(key, column_start=starting_ts,
                column_finish=int(time.time()) - RATE_TAIL_DELAY)
    value_objects = []
    for r in ret:
        ts = r[0]
        is_valid = r[1]['is_valid']
        val = r[1]['val']
        if is_valid == 0:
            continue
        data = RawData(device, None, oid, path, ts, val, base_freq)
        value_objects.append(data)
    
    # Iterate over the rollup freqencies and then iterate through list of data
    # objects to do the appropriate updates.
    for freq in _get_rollup_freqs(oid):
        for vo in value_objects:
            db.update_stat_aggregation(vo, _agg_timestamp(vo, freq), freq)
    
db.close()
db.stats.report('all')
        